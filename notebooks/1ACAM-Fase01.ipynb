{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Esse caderno tem por objetivo a criação de um modelo básico de treinamento, utilizando o classificador [RandomForest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), que faça a predição de homologação de arquivamentos de procedimentos enviados à 1A.CAM do MPF.\n",
    "\n",
    "Nesse modelo vamos passar a considerar os textos das íntegras.\n",
    "\n",
    "A primeira passagem, serão criadas colunas com as palavras-chaves informadas pela área negocial que vão indicar a ausência ou presença do termo no texto da íntegra da peça de promoção do arquivamento.\n",
    "\n",
    "Na segunda passagem, vamos passar a considerar unicamente o texto da íntegra.\n",
    "\n",
    "**Nota**: os dados desse modelo foram recuperados de procedimentos que tiveram suas deliberações realizadas após o dia 02/07/2018, data em que a nova composição tomou posse na 1A.CAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contagem de termos mais frequentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos fazer uma análise dos textos das íntegras e tentar encontrar os termos mais frequentes.\n",
    "\n",
    "O objetivo é:\n",
    "\n",
    "- fazer uma confirmação das frequências dos termos com a lista de palavras-chaves que nos foi enviada;\n",
    "- descobrir outras palavras que possam ser indicativos de homologação ou não.\n",
    "\n",
    "Serão realizadas três análises:\n",
    "\n",
    "- considerando todas as íntegras;\n",
    "- considerando apenas as íntegras das peças que tiveram seu homologamento confirmado;\n",
    "- considerando os não homologados;\n",
    "\n",
    "Dessa análise, podemos criar colunas derivadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de dados e pré-processamento\n",
    "\n",
    "Vamos fazer a carga dos dados.\n",
    "\n",
    "Vamos ler os textos das íntegras, limpá-los e associar ao dataframe com os dados de homologação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/andrethiago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/andrethiago/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# carga dos textos\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = stopwords.words('portuguese')\n",
    "stop_words.extend(['n', 'nº', 'n°', 'n.º', ',', '.', '!', '?', ';', ':', '...', 'º', '–', '/', '(', ')'])\n",
    "\n",
    "def get_text(file):\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        data=' '.join(line.strip() for line in f)\n",
    "        f.close()\n",
    "    \n",
    "    return data.strip()\n",
    "\n",
    "def get_text_non_stop_words(text):\n",
    "    filtered_text = ''\n",
    "    for w in word_tokenize(text, language='portuguese'):\n",
    "        if w not in stop_words:\n",
    "            filtered_text += w\n",
    "            filtered_text += ' '            \n",
    "    return filtered_text.strip()   \n",
    "    \n",
    "\n",
    "folder_integras = f'{PATH}/integras-textos'\n",
    "\n",
    "#onlyfiles = [f for f in listdir(folder_integras) if isfile(join(folder_integras, f))]\n",
    "\n",
    "texts = {}\n",
    "for file in listdir(folder_integras):\n",
    "    if isfile(join(folder_integras, file)):\n",
    "        texts[file.split('.')[0]] = get_text(join(folder_integras,file))\n",
    "    else:\n",
    "        print('is not file', file)\n",
    "\n",
    "    \n",
    "#texts['64280538']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homologado</th>\n",
       "      <th>id</th>\n",
       "      <th>peca_promocao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>1</td>\n",
       "      <td>63581056</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>1</td>\n",
       "      <td>84000484</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4759</th>\n",
       "      <td>1</td>\n",
       "      <td>69279080</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5386</th>\n",
       "      <td>1</td>\n",
       "      <td>65122401</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>1</td>\n",
       "      <td>66738614</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      homologado        id peca_promocao\n",
       "4972           1  63581056          None\n",
       "1081           1  84000484          None\n",
       "4759           1  69279080          None\n",
       "5386           1  65122401          None\n",
       "1276           1  66738614          None"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# montando um DataFrame\n",
    "df_original = pd.read_json(f'{PATH}/1A.CAM.homologacao-arquivamento.json')\n",
    "df_work = df_original.copy()\n",
    "df_work.dropna(subset=['identificadorPecaPromocao'], inplace=True)\n",
    "df_work.reset_index(drop=True, inplace=True)\n",
    "cols = ['id', 'homologado']\n",
    "df_work.drop(inplace=True, columns=[col for col in df_work.columns if col not in cols])\n",
    "df_work['peca_promocao'] = None\n",
    "df_work.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>homologado</th>\n",
       "      <th>id</th>\n",
       "      <th>peca_promocao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>1</td>\n",
       "      <td>78580922</td>\n",
       "      <td>MINISTÉRIO PÚBLICO FEDERAL PROCURADORIA DA REP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5132</th>\n",
       "      <td>1</td>\n",
       "      <td>67296888</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>1</td>\n",
       "      <td>69926981</td>\n",
       "      <td>MINISTÉRIO PÚBLICO FEDERAL  Procuradoria da Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>1</td>\n",
       "      <td>70438817</td>\n",
       "      <td>Ofício MPF/PRPE/AT nº 171 /2003  Notícia de Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>1</td>\n",
       "      <td>73584328</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      homologado        id                                      peca_promocao\n",
       "2594           1  78580922  MINISTÉRIO PÚBLICO FEDERAL PROCURADORIA DA REP...\n",
       "5132           1  67296888                                                   \n",
       "4541           1  69926981  MINISTÉRIO PÚBLICO FEDERAL  Procuradoria da Re...\n",
       "4031           1  70438817  Ofício MPF/PRPE/AT nº 171 /2003  Notícia de Fa...\n",
       "2670           1  73584328                                                   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Associando o texto...\n",
    "for key, text in zip(texts.keys(), texts.values()):\n",
    "    if key != '':\n",
    "        df_work.loc[df_work.id == int(key), 'peca_promocao'] = text\n",
    "    \n",
    "df_work.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palavras-chaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUSÊNCIA DE IRREGULARIDADES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IRREGULARIDADE SANADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NÃO COMPROVAÇÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INEXISTÊNCIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SOLUCIONADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EXAURIMENTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EXAURIDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PERDA DE OBJETO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IRREGULARIDADE NÃO COMPROVADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INEXISTÊNCIA DE INDÍCIOS DE IRREGULARIDADE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IRREGULARIDADE NÃO CONFIRMADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RECOMENDAÇÃO ATENDIDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RECOMENDAÇÃO ACATADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CUMPRIDAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ADOTADAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>INEXISTÊNCIA DE INDÍCIOS MÍNIMOS DE IRREGULARI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RECOMENDAÇÃO EXPEDIDA E ACATADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>JUSTIFICATIVAS SATISFATÓRIAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CORREÇÃO DAS FALHAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AÇÃO AJUIZADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>QUESTÃO JUDICIALIZADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BANCO DE PREÇOS + RECOMENDAÇÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DIREITO INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>EXCESSO DE PESO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MAMOGRAFIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SISCAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>REGIME JURÍDICO + CONSELHOS PROFISSIONAIS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      palavras-chaves\n",
       "0                         AUSÊNCIA DE IRREGULARIDADES\n",
       "1                               IRREGULARIDADE SANADA\n",
       "2                                     NÃO COMPROVAÇÃO\n",
       "3                                        INEXISTÊNCIA\n",
       "4                                         SOLUCIONADA\n",
       "5                                         EXAURIMENTO\n",
       "6                                            EXAURIDA\n",
       "7                                     PERDA DE OBJETO\n",
       "8                       IRREGULARIDADE NÃO COMPROVADA\n",
       "9          INEXISTÊNCIA DE INDÍCIOS DE IRREGULARIDADE\n",
       "10                      IRREGULARIDADE NÃO CONFIRMADA\n",
       "11                              RECOMENDAÇÃO ATENDIDA\n",
       "12                               RECOMENDAÇÃO ACATADA\n",
       "13                                          CUMPRIDAS\n",
       "14                                           ADOTADAS\n",
       "15  INEXISTÊNCIA DE INDÍCIOS MÍNIMOS DE IRREGULARI...\n",
       "16                    RECOMENDAÇÃO EXPEDIDA E ACATADA\n",
       "17                       JUSTIFICATIVAS SATISFATÓRIAS\n",
       "18                                CORREÇÃO DAS FALHAS\n",
       "19                                      AÇÃO AJUIZADA\n",
       "20                              QUESTÃO JUDICIALIZADA\n",
       "21                     BANCO DE PREÇOS + RECOMENDAÇÃO\n",
       "22                                 DIREITO INDIVIDUAL\n",
       "23                                    EXCESSO DE PESO\n",
       "24                                         MAMOGRAFIA\n",
       "25                                             SISCAN\n",
       "26          REGIME JURÍDICO + CONSELHOS PROFISSIONAIS"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carga das palavras chaves\n",
    "\n",
    "palavras_chaves = pd.read_csv(f'{PATH}/1A.CAM.palavras-chaves.txt')\n",
    "palavras_chaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busca de palavras-chaves nos textos das peças de promoção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada a lista de palavras-chaves anteriormente listada, vamos procurá-la nos textos das íntegras.\n",
    "\n",
    "Vamos criar uma coluna para cada uma das palavras-chaves e marcar com ausência ou presença da mesma no texto correspondente.\n",
    "\n",
    "Vamos utilzar expressões regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7fceb74c0549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpalavras_chaves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c932fd376548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    818\u001b[0m         X = sp.csr_matrix((values, j_indices, indptr),\n\u001b[1;32m    819\u001b[0m                           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                           dtype=self.dtype)\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mcheck_format\u001b[0;34m(self, full_check)\u001b[0m\n\u001b[1;32m    160\u001b[0m             raise ValueError(\"index pointer size (%d) should be (%d)\" %\n\u001b[1;32m    161\u001b[0m                                 (len(self.indptr), major_dim + 1))\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index pointer should start with 0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ver se dá pra usar o sklearn aqui...\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1,3))\n",
    "bag = count.fit_transform(list(texts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [w for w in word_tokens if not w in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['üá qep 7ô w640 tej 61', 'üá qep 7ô w640 tej', 'üá qep 7ô w640', 'üá qep 7ô', 'üá qep', 'üá', 'üsuc ogrto oa rei jc1 iii', 'üsuc ogrto oa rei jc1', 'üsuc ogrto oa rei', 'üsuc ogrto oa']\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "top_ten = nlargest(10, count.vocabulary_, key=count.vocabulary_.get)\n",
    "\n",
    "print(top_ten)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
